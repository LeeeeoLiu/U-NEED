# dataset
dataset: UNEOD
tokenize:
  conv: gpt2
# dataloader
context_truncate: 128
response_truncate: 30
item_truncate: 100
scale: 1
# model
conv_model: GPT2
# optim
conv:
  epoch: 20
  batch_size: 32
  gradient_clip: 1.0
  update_freq: 1
  early_stop: False
  stop_mode: min
  impatience: 3
  optimizer:
    name: AdamW
    lr: !!float 1e-4
  lr_scheduler:
    name: TransformersLinearLR
    warmup_steps: 2000